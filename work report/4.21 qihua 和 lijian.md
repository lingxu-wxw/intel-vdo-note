#### 4.21 qihua and lijian

---

```
w/	with
```

```
Good. Yes, you may look at the end-to-end solution with VDO and QAT compression. Below solutions could be FYI:
1, VDO w/ QAT compression on initiator side: e.g. NVMe-oF initiator, iSCSI-Target initiator. I would suggest NVMe-oF initiator w/ NVMe-oF Target (which could be all-flash array storage pool, as it may match the performance requirement for QAT compression acceleration). The performance benefit for the solution could be from the network bandwidth reduction, and storage IO reduction, and compression acceleration.
2, VDO w/ QAT compression on target side: e.g. NVMe-oF target, iSCSI-Target target. I would suggest NVMe-oF Target due to the same reason above. The performance benefit for the solution could be from storage IO reduction, and compression acceleration.
3, VDO w/ QAT compression in Ceph RBD. It should be all-flash array solution for each Ceph Storage Node. The performance benefit for the solution could be from the network bandwidth reduction, and storage IO reduction, and compression acceleration.
4, VDO w/ QAT compression in Ceph Storage node w/ KV-store (which is based on RockDB). It should be all-flash array solution for Ceph. The performance benefit for the solution could be from storage IO reduction, and compression acceleration.

Above solutions are all based on block layer. They are suitable and easy for VDO to be integrated.
Above solutions all have the corresponding stack supported in SPDK user space lib. So we may also look at them w/ SPDK + QAT Compression. SPDK has supported the block layer user space stack for QAT compression, which is called SPDK compression bdev. It is based on DPDK CompressionDev. It would be future work after the current VDO w/ QAT compression.
```

```
Thanks Qihua, these are great ideas. 
Xinwei, you can work with Jiahui to establish the PoC with help of Qihua and Weigang. 
Thanks you all.
```

```
Added the values for each solution w/ VDO + QAT compression in blue, which can be used in paper.

1, VDO w/ QAT compression on initiator side: e.g. NVMe-oF initiator, iSCSI-Target initiator. I would suggest NVMe-oF initiator w/ NVMe-oF Target (which could be all-flash array storage pool, as it may match the performance requirement for QAT compression acceleration). The performance benefit for the solution could be from the network bandwidth reduction, and storage IO reduction, and compression acceleration.
Value for user:
•	Performance gain
•	Elastic Compute Service (ECS) in Public Cloud: Reduce CPU utilization, so as to free up more CPU for rental.

2, VDO w/ QAT compression on target side: e.g. NVMe-oF target, iSCSI-Target target. I would suggest NVMe-oF Target due to the same reason above. The performance benefit for the solution could be from storage IO reduction, and compression acceleration.
Value for user:
•	Performance gain
•	Storage pool in Public Cloud: Reduce Freq of CPU so as to reduce CTO

3, VDO w/ QAT compression in Ceph RBD. It should be all-flash array solution for each Ceph Storage Node. The performance benefit for the solution could be from the network bandwidth reduction, and storage IO reduction, and compression acceleration.
Value for user:
•	Performance gain
•	Elastic Compute Service (ECS) in Public Cloud: Reduce CPU utilization, so as to free up more CPU for rental.

4, VDO w/ QAT compression in Ceph Storage node w/ KV-store (which is based on RockDB). It should be all-flash array solution for Ceph. The performance benefit for the solution could be from storage IO reduction, and compression acceleration.
 Value for user:
•	Performance gain
•	Storage pool in Public Cloud: Reduce Freq of CPU so as to reduce CTO
```

```
Hi Qihua, 
NVMe-oF is not widely accepted, and the standard is not stable enough. Instead, DPDK + RDMA （RoCE）is widely accepted as nbdev. 
May we setup VDO with mature SPDK tech? Otherwise, iscsi can provide better/equal performance in comparison to RDMA?
If necessary, we can schedule a zoom meeting to discuss in detail. 
Bests, 
Jian
```

```
Hi Professor Li,

Sure, we can have the meeting to discuss. I can invite SPDK expert for this topic.
Let me schedule a meeting later.
```

```
Hi Qihua, 
Is it possible to establish the diskless server as mentioned in the paper attached? That is interesting for public cloud to save one disk per server. Moreover, we can setup with the smartnic with virtio-blk (hardware offload backend) ,  to establish the whole ecosystem ready-to use for a cloud provider. 
Thanks.
```

