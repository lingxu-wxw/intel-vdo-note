#### 5.6 Ceph搭建

---

```
pvcreate /dev/mapper/vdo0
vgcreate vg0 /dev/mapper/vdo0
lvcreate -l 100%FREE -n lv0 vg0
这样可以生成一个逻辑卷

然后
ceph-deploy osd create ceph1 --data vg0/lv0
这个create就是prepare+activate

ceph到这个luminous就不支持这种multipath设备，他们认为LVM的管理不应该由他们负责
所以就只能我们先创建好再搭建OSD

 继续使用的话，在上面搭程序跑其他数据，我目前学会的方法是创造一个rbd设备使用osd
 
 创造一个pool：ceph osd pool create vdo_pool 128
创造rbd设备：rbd create vdo_rbd -p vdo_pool --size 10240
关闭不需要的功能：rbd feature disable vdo_rbd -p vdo_pool exclusive-lock, object-map, fast-diff, deep-flatten
映射rbd设备：rbd map vdo_rbd -p vdo_pool
然后你就会得到一个块设备/dev/rbd0，这个设备和/dev/mapper/vdo0那种一样，可以mkfs.xfs -K初始化然后mount挂载再执行测试
```

```
完整版
mkdir myceph
cd myceph
ceph-deploy new qat-helin-1
vim ceph.conf

[global]
osd pool default size = 1
osd pool default min size = 1

ceph-deploy install --release luminous qat-helin-1
ceph-deploy mon create-initial
ceph-deploy admin qat-helin-1
ceph-deploy  mgr create qat-helin-1

vdo create --name=vdo0 --device=/dev/nvme0n1p1

pvcreate /dev/mapper/vdo0
vgcreate vg0 /dev/mapper/vdo0
lvcreate -l 100%FREE -n lv0 vg0
ceph-deploy osd create qat-helin-1 --data vg0/lv0

ceph osd pool create vdo_pool 128
rbd create vdo_rbd -p vdo_pool --size 10240
rbd feature disable vdo_rbd -p vdo_pool exclusive-lock, object-map, fast-diff, deep-flatten
rbd map vdo_rbd -p vdo_pool

mkfs.xfs -K /dev/rbd0
mount /dev/rbd0 /mnt/VDOVolume/
```

